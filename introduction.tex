
\section{Introduction}

Database transactions allow users to group operations on multiple
objects into a single logical unit, equipped with a set of four key
properties - atomicity, consistency, isolation, and durability (ACID).
Concurrency control mechanisms provide specific instantiations of
these properties to yield different ACID variants that characterize
how and when the effects of concurrently executing transactions become
visible to one another.  \emph{Serializability} is a particularly
well-studied instantiation that imposes strong atomicity and isolation
constraints on transaction execution, ensuring that any permissible
concurrent schedule yields results equivalent to a serial one in which
there is no interleaving of actions from different transactions.

The guarantees provided by serializability do not come for free,
however - pessimistic concurrency control methods require databases to
use expensive mechanisms such as two-phase locking that incur overhead
to deal with deadlocks, rollbacks, and
re-execution~\cite{twopl,ullmanbook}.  Similar criticisms apply to
optimistic multi-version concurrency control methods that must deal
with timestamp and version management~\cite{BG81}.  These issues are
further exacerbated when the database is replicated, requiring
additional coordination
mechanisms~\cite{cap,sernotavlbl,bailishat,bernsigmod13}.

Because serializable transactions favor correctness over performance,
there has been long-standing interest~\cite{gray1976} in the database
community to consider weaker variants that try to recover performance,
even at the expense of simplicity and ease of reasoning.  These
instantiations permit a transaction to witness various effects of
newly committed, or even concurrently running, transactions while it
executes, thus weakening serializability's strong isolation
guarantees.  The ANSI SQL 92 standard defines three such weak
isolation levels which are now implemented in many relational and
NoSQL databases. Not surprisingly, weakly-isolated transactions have
been found to significantly outperform serializable transactions on
benchmark suites, both on single-node databases and multi-node
replicated stores~\cite{dbtuningbook,bailishat,bailisvldb}, leading to
their overwhelming adoption. A 2013 study~\cite{bailishotos} of 18
popular ACID and ``NewSQL'' databases found that only three of them
offer serializability by default, and half, including Oracle 11g, do
not offer it at all.  A 2015 study~\cite{bailisferal} of a large
corpus of database applications finds no evidence that applications
manifestly change the default isolation level offered by the
database. Taken together, these studies make clear that
weakly-isolated transactions are quite prevalent in practice, and
serializable transactions are often eschewed.

Unfortunately, weak isolation admits behaviors that are difficult to
comprehend~\cite{berenson}. To quantify weak isolation anomalies,
Fekete \emph{et al.}~\cite{feketevldb09} devised and experimented with
a microbenchmark suite that executes transactions under a
weakly-isolated \emph{read committed} isolation level - the default
level for 8 of the 18 databases studied in~\cite{bailishotos}, and
found that 25 out of every 1000 rows in the database violate at least
one integrity constraint. Bailis \emph{et al.}~\cite{bailisferal} rely
on Rails' \emph{uniqueness validation} to maintain uniqueness of
records while serving Linkbench's~\cite{linkbench} insertion workload
(6400 records distributed over 1000 keys; 64 concurrent clients), and
report discovering more than 10 duplicate records.  Rails relies on
database transactions to validate uniqueness during insertions, which
is sensible if transactions are serializable, but incorrect under the
weak isolation level used in the experiments. The same study has found
that 13\% of all invariants among 67 open source Ruby-on-Rails
applications are at risk of being violated due to weak
isolation. Indeed, incidents of safety violations due to executing
applications in a weakly-isolated environment have been reported on
web services in production~\cite{starbucksbug, scimedbug}, including
in safety-critical applications such as bitcoin
exchanges~\cite{poloniexbug, bitcoinbug}. While enforcing
serializability for all transactions would be sufficient to avoid
these errors and anomalies, it would likely be an overly conservative
strategy; indeed, 75\% of the invariants studied in~\cite{bailisferal}
were shown to be preserved under some form of weak isolation.  When to
use weak isolation, and in what form, is therefore a prominent
question facing all database programmers.\footnote{This position has
been echoed by leading database researchers who lament the lack of a
better understanding of this problem; see e.g., {\tt
  http://www.bailis.org/blog/understanding-weak-isolation-is-a-serious-problem}.}

A major problem with weak isolation as currently specified is that its
semantics in the context of user programs is not easily
understood. The original proposal~\cite{gray1976} defines multiple
``degrees'' of weak isolation in terms of implementation details such
as the nature and duration of locks held in each case. The ANSI SQL 92
standard defines four levels of isolation (including serializability)
in terms of various undesirable \emph{phenomena} (\eg \emph{dirty
  reads} - reading data written by an uncommitted transaction) each is
required to prevent. While this is an improvement, this style of
definition still requires programmers to be prescient about the
possible ways various undesirable phenomena might manifest in their
applications, and in each case determine if the phenomenon can be
allowed without violating application invariants. This is
understandably hard, especially in the absence of any formal
underpinning to define weak isolation semantics.  Adya~\cite{adyaphd}
presents the first formal definitions of some well-known isolation
levels in the context of a sequentially consistent (SC) database.
However, there has been little progress relating Adya's system model
to a formal operational semantics or a proof system that can
facilitate rigorous correctness arguments.  Consequently, reasoning
about weak isolation remains an error prone endeavor, with major
database vendors~\cite{postgresiso, mysqliso, oracleiso} continuing to
document their isolation levels primarily in terms of the undesirable
phenomena a particular isolation level may induce, placing the burden
on the programmer to determine application correctness.

Recent results on reasoning about application invariants in the
presence of weak consistency~\cite{burckhardt14, redblueosdi,
  redblueatc, ecinec, gotsmanpopl16} address broadly related concerns.
Weak consistency is a phenomenon that manifests on replicated data
stores, where atomic operations are concurrently executed against
different replicas, resulting in an execution order inconsistent with
any sequential order. In contrast, weak isolation is a property of
concurrent transactions interfering with one another, resulting in an
execution order that is not serializable. Unlike weak consistency,
weak isolation can manifest even in an unreplicated setting, as
evident from the support for weakly-isolated transactions on
conventional (unreplicated) databases as mentioned above.

%% In the
%% presence of replication, however, the interaction between weak
%% isolation and weak consistency can be subtle and non-trivial.
%% Understanding weak isolation in these varied contexts thus requires
%% new insights and substantial generalization of existing techniques.

%% Recent results on reasoning about application invariants in the
%% presence of weak consistency~\cite{burckhardt14, redblueosdi,
%% redblueatc, ecinec, gotsmanpopl16} address broadly related concerns.
%% Weak consistency is a phenomenon that usually manifests on replicated
%% data stores, where operations are concurrently executed against
%% different replicas, resulting in an order of execution inconsistent
%% with their serial order. The operations, nonetheless, are atomic and
%% fully isolated, and each operation is required to locally preserve
%% application invariants. In contrast, weak isolation is a property of
%% transactions, which are sets of atomic operations. Weak isolation
%% manifests when successive atomic operations in a transaction witness
%% different contemporary states of the database (or different replicas
%% of the replicated store) which, although consistent individually, may
%% not be obviously reconciled into a consistent global view.
%% Intuitively, the latter problem reduces to the former if all
%% transactions contain a single operation. Furthermore, weak isolation
%% can exist independent of weak consistency, as evident from the
%% presence of weakly-isolated transactions on conventional RDBS. With an
%% added complexity of replication, richer mechanisms are needed to
%% reason about weak isolation in tandem with weak consistency. Thus,
%% frameworks for reasoning about weak isolation will necessarily have to
%% generalize the reasoning frameworks developed for weak consistency in
%% new and important ways.

% The framework should be general enough to reason about the semantics
% of multiple isolation levels, including those proposed after the SQL
% 92 standard, in the context of various stores (\eg sequentially
% consistent store of~\cite{adyaphd}, causally consistent store
% of~\cite{gotsmanpopl16} etc). 

In this paper, we propose a program logic for weakly-isolated
transactions along with automated verification support to allow
developers to verify the soundness of their applications, without
having to resort to low-level operational reasoning as they are forced
to do currently.  We develop a set of syntax-directed compositional
proof rules that enable the construction of correctness proofs for
transactional programs in the presence of a weakly-isolated
concurrency control mechanism.  Realizing that the proof burden
imposed by these rules may discourage applications programmers from
using them, we also present an inference procedure based on these
rules that automatically verifies the weakest isolation level for a
transaction that still ensures its invariants are maintained.  The key
to inference is a novel formulation of database state (represented as
sets of tuples) as a monad, and in which database computations are
interpreted as state transformers over these sets.  This
interpretation leads to an encoding of database computations amenable
for verification by off-the-shelf SMT solvers.  The paper makes the
following contributions:
%% We have realized these ideas as an embedded DSL within OCaml, that
%% supports common relational SQL operations (updates, selects, inserts,
%% deletes, joins, etc.).  Experimental results on real-world benchmarks
%% demonstrate the feasibility and value of automated verification for
%% weakly isolated transactions, an important advance in improving the
%% safety of realistic database computations.
\begin{enumerate}
\item We analyze properties of weak isolation in the context of an
    embedded DSL for OCaml that treats SQL-based relational database
    operations (e.g., inserts, selects, deletes, joins, updates, etc.)
    as monadic computations over an abstract database state.
 \item We develop an operational semantics and a compositional
   rely-guarantee style proof system for this language capable of
   relating high-level application invariants to database state,
   parameterized by a weak isolation semantics that selectively
   exposes the visibility of these operations to other transactions.
 \item Exploiting the monadic definition of database state, we devise
   an inference algorithm capable of discovering the weakest isolation
   level that is sound with respect to a transaction's high-level
   consistency requirements.  The algorithm interprets database
   operations as state transformers expressed in a language amenable
   for translation into a decidable fragment of first-order logic, and
   is thus suitable for automated verification using off-the-shelf SMT
   solvers.
 \item We present details of a realistic implementation along with an
   evaluation study on real-world database benchmarks that justify our
   approach and the utility of our proof methodology and inference
   mechanism.
\end{enumerate}
\noindent Our results provide the first (to the best of our knowledge)
formalization of weakly-isolated transactions, along with an
expressive and compositional proof automation framework capable of
verifying the safety of high-level consistency conditions attached to
these transactions.  Collectively, these contributions allow
weakly-isolated transactions to enjoy the same rigorous reasoning
capabilities as their strongly-isolated (serializable) counterparts.

The remainder of the paper is organized as follows. The next section
provides motivation and background on serializable and weakly-isolated
transactions. \S\ref{sec:opsem} presents an operational semantics for
a core language that supports weakly-isolated transactions,
parameterized over different isolation notions. \S\ref{sec:reasoning}
formalizes the proof system that we use to reason about program
invariants, and establishes the soundness of these rules with respect
to the semantics. \S\ref{sec:inference} describes the inference
algorithm, and the state transformer encoding.  We describe our
implementation in \S\ref{sec:implementation}, and provide case studies
and benchmark results in \S\ref{sec:case-studies}.  Related work is
given in \S\ref{sec:relatedwork}, and \S\ref{sec:conclusions} presents
conclusions.
