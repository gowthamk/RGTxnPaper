
\section{Introduction}

Database transactions allow users to group operations on multiple
objects into a single logical unit, equipped with a set of four key
properties - atomicity, consistency, isolation, and durability (ACID).
Concurrency control mechanisms provide specific instantiations of
these properties to yield different ACID variants that characterize
how and when the effects of concurrently executing transactions become
visible to one another.  \emph{Serializability} is a particularly
well-studied instantiation that imposes strong atomicity and isolation
constraints on transaction execution, ensuring that any permissible
concurrent schedule yields results equivalent to a serial one in which
there is no interleaving of actions from different transactions.

The guarantees provided by serializability do not come for free,
however - pessimistic concurrency control methods require databases to
use expensive mechanisms such as two-phase locking that incur overhead
to deal with deadlocks, rollbacks, and
re-execution~\cite{twopl,ullmanbook}.  Similar criticisms apply to
optimistic multi-version concurrency control methods that must deal
with timestamp and version management~\cite{BG81}.  These issues are
further exacerbated when the database is replicated, requiring
additional coordination
mechanisms~\cite{cap,sernotavlbl,bailishat,bernsigmod13}.

The tension between serializable transactions, which are easy to
reason about but difficult to implement efficiently, and more
pragmatic variants that are driven by performance and availability
considerations, has motivated the development of weaker forms of
transaction isolation, beginning as early as 1976~\cite{gray1976}. The
ANSI SQL 92 standard defines three such weak isolation levels which
are now implemented in many relational and NoSQL databases. Not
surprisingly, weakly-isolated transactions have been found to
significantly outperform serializable transactions on benchmark
suites, both on single-node databases and multi-node replicated
stores~\cite{dbtuningbook,bailishat,bailisvldb}, leading to their
overwhelming adoption. A 2013 study~\cite{bailishotos} of 18 popular
ACID and ``NewSQL'' databases found that only three of them offer
serializability by default, and half, including Oracle 11g, do not
offer it at all.  A 2015 study~\cite{bailisferal} of a large corpus of
database applications finds no evidence that applications manifestly
change the default isolation level offered by the database. Taken
together, these studies make clear that weakly-isolated transactions
are quite prevalent in practice, whereas serializable transactions are
often eschewed.

Unfortunately, weak isolation admits behaviors that are difficult to
comprehend~\cite{berenson}. To quantify weak isolation anomalies,
Fekete \emph{et al.}~\cite{feketevldb09} devised and experimented with
a microbenchmark suite that executes transactions under a
weakly-isolated \emph{read committed} isolation level - the default
level for 8 of the 18 databases studied in~\cite{bailishotos}, and
found that 25 out of every 1000 rows in the database violate at least
one integrity constraint. Bailis \emph{et al.}~\cite{bailisferal} rely
on Rails' \emph{uniqueness validation} to maintain uniqueness of
records while serving Linkbench's~\cite{linkbench} insertion workload
(6400 records distributed over 1000 keys; 64 concurrent clients), and
report discovering more than 10 duplicate records.  Rails relies on
database transactions to validate uniqueness during insertions, which
is sensible if transactions are serializable, but incorrect under the
\emph{read committed} isolation level used in the experiments. The
same study has found that 13\% of all invariants among 67 open source
Ruby-on-Rails applications are at risk of being violated due to weak
isolation. Indeed, incidents of safety violations due to executing
applications in a weakly-isolated environment have been reported on
web services in production~\cite{starbucksbug, scimedbug}, including
in safety-critical applications such as bitcoin
exchanges~\cite{poloniexbug, bitcoinbug}. While enforcing
serializability for all transactions would be sufficient to avoid
these errors and anomalies, it would likely be an overly conservative
strategy; indeed, 75\% of the invariants studied in~\cite{bailisferal}
were shown to be preserved under some form of weak isolation.  When to
use weak isolation, and in what form, is therefore a prominent
question facing all database programmers.\footnote{This position has
  been echoed by leading database researchers who lament the lack of
  better understanding of this problem.  See: {\tt
    http://www.bailis.org/blog/understanding-weak-isolation-is-a-serious-problem}.}

A major problem with weak isolation is that its semantics in the
context of user programs is not well-understood. The original
proposal~\cite{gray1976} defines multiple ``degrees'' of weak
isolation in terms of implementation details such as the nature and
duration of locks held in each case. The ANSI SQL 92 standard defines
four levels of isolation (including serializability) in terms of
various undesirable \emph{phenomena} (\eg \emph{dirty reads}) each is
required to prevent. While this is an improvement, it requires
programmers to be prescient about the possible ways various
undesirable phenomena might manifest in their applications, and in
each case determine if the phenomenon can be allowed without violating
application invariants. This is understandably hard, especially in the
absence of any formal underpinning to define weak isolation semantics.
Adya~\cite{adyaphd} presents the first formal definitions of some
well-known isolation levels in the context of a sequentially
consistent (SC) database.  However, there has been little progress
relating Adya's system model to a formal operational semantics or a
proof system that can facilitate rigorous correctness arguments.
Consequently, reasoning about weak isolation remains an error prone
endeavor, with major database vendors~\cite{postgresiso, mysqliso,
oracleiso} continuing to document their isolation levels primarily in
terms of the undesirable phenomena a particular isolation level may
induce, placing the burden on the programmer to determine application
correctness.

Recent results on reasoning about application invariants in the
presence of weak consistency~\cite{burckhardt14, redblueosdi,
redblueatc, ecinec, gotsmanpopl16} address broadly related concerns.
Weak consistency is a phenomenon that manifests on replicated data
stores, where atomic operations are concurrently executed against
different replicas, resulting in an execution order inconsistent with
any sequential order. In contrast, weak isolation is a property of
concurrent transactions interfering with one another resulting in an
execution order that is not serializable. Unlike weak consistency,
weak isolation can manifest even in an unreplicated setting, as
evident from the support for weakly-isolated transactions on
conventional (unreplicated) databases as mentioned above.

%% In the
%% presence of replication, however, the interaction between weak
%% isolation and weak consistency can be subtle and non-trivial.
%% Understanding weak isolation in these varied contexts thus requires
%% new insights and substantial generalization of existing techniques.

%% Recent results on reasoning about application invariants in the
%% presence of weak consistency~\cite{burckhardt14, redblueosdi,
%% redblueatc, ecinec, gotsmanpopl16} address broadly related concerns.
%% Weak consistency is a phenomenon that usually manifests on replicated
%% data stores, where operations are concurrently executed against
%% different replicas, resulting in an order of execution inconsistent
%% with their serial order. The operations, nonetheless, are atomic and
%% fully isolated, and each operation is required to locally preserve
%% application invariants. In contrast, weak isolation is a property of
%% transactions, which are sets of atomic operations. Weak isolation
%% manifests when successive atomic operations in a transaction witness
%% different contemporary states of the database (or different replicas
%% of the replicated store) which, although consistent individually, may
%% not be obviously reconciled into a consistent global view.
%% Intuitively, the latter problem reduces to the former if all
%% transactions contain a single operation. Furthermore, weak isolation
%% can exist independent of weak consistency, as evident from the
%% presence of weakly-isolated transactions on conventional RDBS. With an
%% added complexity of replication, richer mechanisms are needed to
%% reason about weak isolation in tandem with weak consistency. Thus,
%% frameworks for reasoning about weak isolation will necessarily have to
%% generalize the reasoning frameworks developed for weak consistency in
%% new and important ways.

% The framework should be general enough to reason about the semantics
% of multiple isolation levels, including those proposed after the SQL
% 92 standard, in the context of various stores (\eg sequentially
% consistent store of~\cite{adyaphd}, causally consistent store
% of~\cite{gotsmanpopl16} etc). 

In this paper, we propose a program logic for weakly-isolated
transactions along with automated verification support that realizes
this goal.  We develop a set of syntax-directed compositional proof
rules that allow developers to build correctness proofs for
transactional programs in the presence of a weakly-isolated
concurrency control mechanism.  Moreover, we also present an inference
algorithm based on a state transformer semantics that allows us to
automatically verify the weakest isolation level for a transaction
that ensures its consistency requirements are maintained.  The key to
inference is our formulation of database state (represented as sets of
tuples) as a monad, in which database computations are interpreted as
transformers over these sets.  This interpretation leads to an
encoding database operations amenable for verification by SMT solvers.

We have realized these ideas as an embedded DSL within OCaml, with
support for common SQL operations (updates, selects, inserts, deletes,
joins, etc.).  Experimental results on real-world benchmarks
demonstrate the feasibility of automated verification of weakly
isolated transactions, an important advance in improving the safety of
realistic database computations. The paper makes the following
contributions:
\vspace*{-6pt}
\begin{enumerate}
\item An operational semantics for an embedded DSL in OCaml that
  expresses SQL-based relational database operations as monadic
  computations on database state represented abstractly as sets of
  tuples.
 \item A compositional proof system for this language capable of
   relating high-level application invariants to transformations on
   database state, parameterized by a weak isolation semantics that
   selectively exposes the visibility of these operations to other
   transactions.
 \item An inference algorithm capable of discovering the
   weakest isolation level that is sound with respect to a
   transaction's high-level consistency requirements.  The algorithm
   exploits our monadic formulation, treating database computations as
   predicate state transformers, that leads to a natural encoding
   amenable for automated verification.
 \item A realistic implementation along with results on real-world
   database benchmarks that justify our approach and the utility of
   our proof methodology.
\end{enumerate}
\noindent Our results provide the first (to the best of our knowledge)
automated mechanism to verify the safety of high-level consistency
conditions for weakly-isolated transactions, thereby allowing such
transactions to enjoy the same rigorous reasoning capabilities as
their strongly-isolated (serializable) counterparts.

The remainder of the paper is organized as follows. The next section
provides motivation and background on serializable and weakly-isolated
transactions. \S\ref{sec:opsem} presents an operational semantics for
a core language that supports weakly-isolated transactions,
parameterized over different isolation notions. \S\ref{sec:reasoning}
formalizes the proof system that we use to reason about program
invariants, and establishes the soundness of these rules with respect
to the semantics. \S\ref{sec:inference} describes the inference
algorithm, and the state transformer encoding.  We describe our
implementation, benchmark results, and case studies in
\S\ref{sec:case-studies}.  Related work and conclusions are given in
\S\ref{sec:relatedwork}.
