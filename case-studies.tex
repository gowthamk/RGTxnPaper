\section{Case Studies}
\label{sec:case-studies}

In this section, we present two case studies that demonstrate the
applicability of our reasoning approach in varied settings, and the
consequent practical utility.

\paragraph{A video view counter} Our first application is a
Ruby-on-Rails implementation of a counter to count the number of views
a video has garnered on a video-sharing website like
YouTube\footnote{Source code of the applications, experiments, and the
tools we developed to analyze Ruby can be found at the following
anonymized url: \url{...}}. The application stores the video count in
a Postgres database, and supports a read operation (a database read
transaction), and an increment operation (a read-increment-write
sequence wrapped in a transaction). The application is expected to
preserve the \emph{monotonicity invariant}, namely that the view count
should never appear to be decreasing. However, the default {\sc rc}
isolation of Postgres may lead to anomalous executions as confirmed by
our experiments; with 16 (resp.  32) concurrent writers performing
single increments, we observed a total of 7 (17) violations of
monotonicity. We subsequently instantiated our reasoning framework for
an {\sc sc} store with Postgres isolation levels (which are stronger
than ANSI SQL's and MySQL's), and spent 5 man hours determining
appropriate isolation levels needed to enforce the
invariant\footnote{The representative \txnimp programs we used to
formally reason about scenarios described in this section can be found
in the supplementary.} In particular, we were able to formally prove
that to preserve the invariant, writers need to execute at
\iso{Repeatable Read} isolation, while readers can continue at {\sc
rc}. Repeating the experiments with this configuration led to no
violations, as expected. While executing all transactions at {\sc ser}
also prevents violations, it led to an additional 47\% increase in
latency on an average (16 concurrent writers; 5 rounds). We repeated
the exercise for MySQL, and found that its {\sc rr} is not sufficient
to enforce monotonicity; {\sc ser} is needed. 

% To understand whether a straightforward implementation of the view
% counter preserves the invariant, we considered a \txnimp program
% that is representative of the counter application. In particular, we
% considered four concurrent transactions that operate on a shared
% counter: a reader transaction that reads the counter twice obtaining
% the values $k_1$ and $k_2$, respectively, and three writer
% transactions, each incrementing the counter once. If the reader
% witnesses a monotonically increasing view count, then $k_2\ge k_1$.
% We then instantiated our reasoning framework for an {\sc sc} store
% with {\sc rc} as the default isolation level (Postgres is one such
% store), with the aim of verifying the program under this
% instantiation.  Our attempts were however unsuccessful owing to our
% inability to prove that a writer transaction always commits a value
% no smaller than the current value of the counter. This points to the
% existence of anamolous executions where  To understand the
% prevalence of such anomalies, we implemented a view counter
% application in Ruby-on-Rails using standard templates and
% techniques.   
To understand the effect of changing the store consistency, we
re-implemented the counter as a \emph{replicated data type}
(RDT)~\cite{crdt,burckhardt14} on top of Cassandra
\emph{bolted-on}~\cite{bailisbolton} with {\sc cc} and {\sc sc}
consistency levels, and ANSI SQL isolation levels. Keeping the
consistency level at {\sc ec}, we then repeated our experiments with
committed reads and serializable increments - a combination which was
proved to preserve monotonicity on an {\sc sc} store, but nonetheless
observed monotonicity violations. On the other hand, instantiating the
store consistency with {\sc cc} allowed us to obtain a proof, and
recover the monotonicity invariant. We in fact obtained a proof after
relaxing the serializability requirement for writes, by changing the
definition of the $\interp{\E}(X)$ (\S\ref{sec:opsem}) to return the
value of the largest write-to-$X$ instead of the latest write-to-$X$.
We implemented this version of the counter RDT, and obtained a 52\%
improvement in latency compared to the previous
implementation\footnote{The trade-off is that the view count is now no
longer accurate, although it grows monotonically.}.

\paragraph{Microblog}

Next, we focus our attention on a Twitter-like microblogging site that
is one the sample applications shipped with a popular textbook on
Ruby-on-Rails~\cite{railsbook}. One of the invariants that the
application chooses to enforce is the referential integrity between
microposts and authors. The application enforces this invariant
entirely in the user space by relying on database-backed transactions
(this behaviour is typical of Rails applications~\cite{bailisferal}).
For instance, deletion of a user is carried out in a transaction that
also includes the deletion of microposts authored by the user.
However, concurrent observers can nonetheless witness violations of
referential integrity. To measure the likelihood of such anomalies in
practice, we first populated a Postgres database with 1000 user
accounts, each user with 50 microposts and 20 followers, chosen
uniformly at random. We then constructed experiments with 64
concurrent clients performing deletions and timeline reads of randomly
chosen users in 1:7 ratio, and witnessed a maximum of 250 violations
of referential integrity. We then instantiated our framework for an
{\sc sc} store with various ANSI SQL isolation levels for user
deletions and timeline reads, and obtained a proof that {\sc rc} is
sufficient for deletions, whereas reads require {\sc rr}. The proof is
immediately applicable for MySQL, which implements ANSI SQL isolation
levels. Since Postgres's isolation levels are stronger than the
corresponding ANSI levels, \rulelabel{RG-Conseq} rule
(Fig.~\ref{fig:rg-rules}) lets us deduce that the same choice of
isolation levels also work on Postgres. Indeed, experiments confirmed
this judgment.

As evident from our case studies, the there already exists a
significant diversity among the weak consistency and isolation
variants implemented on various real-world data stores. More such
variants have been developed in the recent years to meet the need for
high availability on geo-replicated stores. For example,
\emph{Parallel Snapshot Isolation}~\cite{psi} was proposed in 2011,
and \emph{Non-Monotonic Snapshot Isolation}~\cite{nmsi} in 2013, both
targeted for replicated stores. As more commercial weakly consistent
stores adopt various kinds of transactional semantics, like the
recently introduced \emph{lightweight transactions} of Cassandra, it
is reasonable to expect new isolation definitions to continue to be
proposed. It may be possible to carefully engineer a reasoning
framework for each combination of consistency and isolation, but such
a strategy would lead to multiple semantic definitions and proof
systems with no obvious way to compare and relate them. Having a
parameterized semantics and a proof framework built on the such
semantics allows us to support these variants as distinct, yet
comparable instantiations, as we demonstrate in our case studies.
%  GK: what does the following sentence mean?
%  As a consequence, we avoid the need for semantics re-engineering
%  every time an application is retargeted to a new platform.




